{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d6bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt saved to soccer_prompt.txt\n"
     ]
    }
   ],
   "source": [
    "soccer_prompt = \"\"\"\n",
    "You are an expert linguistic annotator specializing in soccer (football) commentary and audio transcripts.\n",
    "You will receive a list of English sentences from soccer match commentary, which may include multiple sentences in a single string. Each input is a raw lowercase transcription from live match commentary, post-match analysis, or soccer-related discussions.\n",
    "\n",
    "Your task is crucial and requires precision for soccer domain understanding. For each input string, you must:\n",
    "\n",
    "1. **TOKENIZE:** Split the input into individual words and punctuation (tokens), preserving all elements including soccer-specific terminology, player names, team names, and match events.\n",
    "\n",
    "2. **ASSIGN BIO TAGS:** For each token, assign exactly one BIO tag with soccer domain expertise:\n",
    "   * **SOCCER ENTITY TAGS (Priority):** Identify soccer-specific entities using the provided `ENTITY_TYPES` list.\n",
    "     - `B-<ENTITY_TYPE>` for the *beginning* of an entity phrase (e.g., `B-PLAYER_NAME`, `B-TEAM_NAME`, `B-GOAL`).\n",
    "     - `I-<ENTITY_TYPE>` for *inside* an entity phrase (e.g., `I-PLAYER_NAME`, `I-TEAM_NAME`).\n",
    "   * **COMMENTARY INTENT TAGS (Default/Fallback):** If a token is *not* part of any specific entity, tag it to reflect the overall commentary intent.\n",
    "     - The first non-entity token of the input should be `B-<UTTERANCE_INTENT>`.\n",
    "     - Subsequent non-entity tokens should be `I-<UTTERANCE_INTENT>`.\n",
    "     - The `<UTTERANCE_INTENT>` should be from `INTENT_TYPES`.\n",
    "   * **CRITICAL:** Every token, including punctuation, must have a tag. Use `O` (Outside) if no entity or intent applies.\n",
    "\n",
    "3. **EXTRACT INTENT:** Determine and provide the single overall `intent` of the entire input string from `INTENT_TYPES`, considering the soccer context (e.g., live commentary, analysis, celebration, etc.).\n",
    "\n",
    "4. **OUTPUT FORMAT (CRITICAL):** Return a JSON array of objects. Each object must contain:\n",
    "   * `text`: The original lowercase input string (for verification).\n",
    "   * `tokens`: A JSON array of all tokenized words and punctuation.\n",
    "   * `tags`: A JSON array of BIO tags, exactly matching the `tokens` array in length.\n",
    "   * `intent`: A single string representing the overall commentary intent.\n",
    "\n",
    "**SOCCER ENTITY TYPES LIST (USE ONLY THESE FOR ENTITY TAGS):**\n",
    "[\n",
    "  \"PLAYER_NAME\", \"TEAM_NAME\", \"COACH_NAME\", \"MANAGER_NAME\", \"REFEREE_NAME\", \"ASSISTANT_REFEREE\", \"VAR_REFEREE\",\n",
    "  \"FOURTH_OFFICIAL\", \"GOALKEEPER\", \"DEFENDER\", \"MIDFIELDER\", \"FORWARD\", \"STRIKER\", \"WINGER\", \"CAPTAIN\",\n",
    "  \"SUBSTITUTE\", \"ACADEMY_PLAYER\", \"YOUTH_PLAYER\", \"VETERAN\", \"LEGEND\", \"CLUB_PRESIDENT\", \"DIRECTOR\",\n",
    "  \"GOAL\", \"ASSIST\", \"SHOT\", \"SHOT_ON_TARGET\", \"SHOT_OFF_TARGET\", \"BLOCKED_SHOT\", \"SAVE\", \"CATCH\", \"PUNCH\",\n",
    "  \"YELLOW_CARD\", \"RED_CARD\", \"SECOND_YELLOW\", \"FOUL\", \"PENALTY\", \"PENALTY_MISS\", \"PENALTY_SAVE\", \"OFFSIDE\",\n",
    "  \"SUBSTITUTION\", \"CORNER_KICK\", \"FREE_KICK\", \"DIRECT_FREE_KICK\", \"INDIRECT_FREE_KICK\", \"THROW_IN\",\n",
    "  \"KICK_OFF\", \"OWN_GOAL\", \"HEADER\", \"VOLLEY\", \"BICYCLE_KICK\", \"TACKLE\", \"INTERCEPTION\", \"CLEARANCE\",\n",
    "  \"CROSS\", \"PASS\", \"THROUGH_BALL\", \"BACK_PASS\", \"DRIBBLE\", \"NUTMEG\", \"SKILL_MOVE\", \"RUN\", \"SPRINT\",\n",
    "  \"MATCH_DATE\", \"MATCH_TIME\", \"KICK_OFF_TIME\", \"STADIUM_NAME\", \"VENUE\", \"CAPACITY\", \"ATTENDANCE\",\n",
    "  \"MATCH_SCORE\", \"FINAL_SCORE\", \"HALF_TIME_SCORE\", \"FULL_TIME\", \"HALF_TIME\", \"FIRST_HALF\", \"SECOND_HALF\",\n",
    "  \"EXTRA_TIME\", \"INJURY_TIME\", \"STOPPAGE_TIME\", \"OVERTIME\", \"ADDED_TIME\", \"MATCH_DURATION\",\n",
    "  \"LEAGUE_NAME\", \"TOURNAMENT_NAME\", \"COMPETITION\", \"CHAMPIONSHIP\", \"CUP\", \"FRIENDLY\", \"INTERNATIONAL\",\n",
    "  \"DOMESTIC\", \"CONTINENTAL\", \"WORLD_CUP\", \"EUROS\", \"CHAMPIONS_LEAGUE\", \"EUROPA_LEAGUE\", \"PREMIER_LEAGUE\",\n",
    "  \"LA_LIGA\", \"SERIE_A\", \"BUNDESLIGA\", \"LIGUE_1\", \"MLS\", \"COPA_AMERICA\", \"AFCON\",\n",
    "  \"FORMATION\", \"LINEUP\", \"STARTING_XI\", \"BENCH\", \"SQUAD\", \"TACTIC\", \"STRATEGY\", \"GAME_PLAN\",\n",
    "  \"PRESSING\", \"COUNTER_ATTACK\", \"POSSESSION\", \"PARKING_THE_BUS\", \"HIGH_LINE\", \"LOW_BLOCK\",\n",
    "  \"MATCH_RESULT\", \"WIN\", \"LOSS\", \"DRAW\", \"VICTORY\", \"DEFEAT\", \"TIE\", \"POINTS\", \"RANKING\", \"TABLE_POSITION\",\n",
    "  \"LEAGUE_POSITION\", \"GOAL_DIFFERENCE\", \"GOALS_FOR\", \"GOALS_AGAINST\", \"CLEAN_SHEET\", \"HAT_TRICK\",\n",
    "  \"BRACE\", \"POSSESSION_PERCENTAGE\", \"PASS_ACCURACY\", \"SHOTS_ON_TARGET\", \"CORNERS\", \"FOULS_COMMITTED\",\n",
    "  \"SEASON\", \"FIXTURE\", \"MATCH_DAY\", \"GAME_WEEK\", \"ROUND\", \"GROUP\", \"GROUP_STAGE\", \"KNOCKOUT_STAGE\",\n",
    "  \"QUARTER_FINAL\", \"SEMI_FINAL\", \"FINAL\", \"PLAYOFF\", \"RELEGATION\", \"PROMOTION\", \"TRANSFER_WINDOW\",\n",
    "  \"HOME_TEAM\", \"AWAY_TEAM\", \"HOME_GROUND\", \"AWAY_GROUND\", \"NEUTRAL_VENUE\", \"TRAINING_GROUND\",\n",
    "  \"ACADEMY\", \"CLUB_FACILITY\", \"DRESSING_ROOM\", \"TUNNEL\", \"PITCH\", \"GRASS\", \"ARTIFICIAL_TURF\",\n",
    "  \"BALL\", \"GOAL_POST\", \"CROSSBAR\", \"NET\", \"JERSEY\", \"BOOTS\", \"SHIN_GUARDS\", \"GLOVES\",\n",
    "  \"VAR\", \"GOAL_LINE_TECHNOLOGY\", \"HAWK_EYE\", \"OFFSIDE_LINE\", \"PENALTY_AREA\", \"SIX_YARD_BOX\",\n",
    "  \"CENTER_CIRCLE\", \"CORNER_ARC\", \"TOUCHLINE\", \"GOAL_LINE\",\n",
    "  \"INJURY\", \"INJURY_TIME_OUT\", \"MEDICAL_TIMEOUT\", \"STRETCHER\", \"CONCUSSION\", \"HAMSTRING\",\n",
    "  \"ANKLE\", \"KNEE\", \"HEAD_INJURY\", \"FITNESS\", \"STAMINA\", \"PACE\", \"STRENGTH\", \"AGILITY\",\n",
    "  \"TRANSFER\", \"LOAN\", \"CONTRACT\", \"SIGNING\", \"RELEASE_CLAUSE\", \"TRANSFER_FEE\", \"WAGE\",\n",
    "  \"AGENT\", \"NEGOTIATION\", \"MEDICAL_EXAMINATION\", \"ANNOUNCEMENT\",\n",
    "  \"GOLDEN_BOOT\", \"GOLDEN_BALL\", \"PLAYER_OF_THE_MATCH\", \"PLAYER_OF_THE_SEASON\", \"BALLON_DOR\",\n",
    "  \"ROOKIE_OF_THE_YEAR\", \"COACH_OF_THE_YEAR\", \"FAIR_PLAY_AWARD\", \"TOP_SCORER\", \"MOST_ASSISTS\",\n",
    "  \"COMMENTATOR\", \"ANALYST\", \"PUNDIT\", \"BROADCAST\", \"LIVE_STREAM\", \"HIGHLIGHTS\", \"REPLAY\",\n",
    "  \"SLOW_MOTION\", \"CAMERA_ANGLE\", \"MICROPHONE\", \"INTERVIEW\", \"POST_MATCH\", \"PRE_MATCH\",\n",
    "  \"FAN\", \"SUPPORTER\", \"ULTRAS\", \"CHANT\", \"SONG\", \"SCARF\", \"FLAG\", \"BANNER\", \"TIFO\",\n",
    "  \"AWAY_FANS\", \"HOME_FANS\", \"ATMOSPHERE\", \"STADIUM_ATMOSPHERE\", \"CROWD\", \"NOISE\"\n",
    "]\n",
    "\n",
    "**SOCCER INTENT TYPES LIST (USE ONE FOR UTTERANCE INTENT AND FOR DEFAULT TAGS):**\n",
    "[\n",
    "  \"MATCH_INQUIRY\", \"SCORE_REQUEST\", \"PLAYER_STATS_REQUEST\", \"TEAM_INFO_REQUEST\", \"FIXTURE_INQUIRY\",\n",
    "  \"TABLE_POSITION_REQUEST\", \"LEAGUE_STANDINGS_REQUEST\", \"TRANSFER_NEWS_REQUEST\", \"INJURY_UPDATE_REQUEST\",\n",
    "  \"HISTORICAL_DATA_REQUEST\", \"RECORD_INQUIRY\", \"COMPARISON_REQUEST\", \"PREDICTION_REQUEST\",\n",
    "  \"LIVE_COMMENTARY\", \"GOAL_ANNOUNCEMENT\", \"CARD_ANNOUNCEMENT\", \"SUBSTITUTION_ANNOUNCEMENT\",\n",
    "  \"INJURY_UPDATE\", \"SCORE_UPDATE\", \"HALF_TIME_UPDATE\", \"FULL_TIME_UPDATE\", \"MATCH_EVENT_UPDATE\",\n",
    "  \"VAR_DECISION\", \"REFEREE_DECISION\", \"WEATHER_UPDATE\", \"ATTENDANCE_UPDATE\",\n",
    "  \"TACTICAL_ANALYSIS\", \"PLAYER_PERFORMANCE_ANALYSIS\", \"TEAM_PERFORMANCE_ANALYSIS\", \"MATCH_REVIEW\",\n",
    "  \"SEASON_REVIEW\", \"PREDICTION\", \"OPINION\", \"CRITICISM\", \"PRAISE\", \"EVALUATION\", \"ASSESSMENT\",\n",
    "  \"COMPARISON\", \"RANKING\", \"RATING\", \"RECOMMENDATION\",\n",
    "  \"TRANSFER_NEWS\", \"INJURY_NEWS\", \"CONTRACT_NEWS\", \"COACHING_CHANGE\", \"TEAM_NEWS\",\n",
    "  \"LEAGUE_UPDATE\", \"RULE_CHANGE\", \"DISCIPLINARY_ACTION\", \"FINE_ANNOUNCEMENT\", \"SUSPENSION_NEWS\",\n",
    "  \"AWARD_ANNOUNCEMENT\", \"MILESTONE_ANNOUNCEMENT\", \"RETIREMENT_NEWS\", \"DEBUT_ANNOUNCEMENT\",\n",
    "  \"CELEBRATION\", \"EXCITEMENT\", \"DISAPPOINTMENT\", \"FRUSTRATION\", \"ENCOURAGEMENT\", \"MOTIVATION\",\n",
    "  \"CHANT\", \"SING_ALONG\", \"CROWD_PARTICIPATION\", \"FAN_REACTION\", \"EMOTIONAL_EXPRESSION\",\n",
    "  \"SURPRISE\", \"SHOCK\", \"AMAZEMENT\", \"DISBELIEF\",\n",
    "  \"TACTICAL_INSTRUCTION\", \"COACHING_COMMAND\", \"REFEREE_INSTRUCTION\", \"CROWD_DIRECTION\",\n",
    "  \"BROADCAST_INSTRUCTION\", \"CAMERA_DIRECTION\", \"REPLAY_REQUEST\", \"HIGHLIGHT_REQUEST\",\n",
    "  \"VOLUME_CONTROL\", \"CHANNEL_CHANGE\", \"MUTE_REQUEST\",\n",
    "  \"RULE_CLARIFICATION\", \"DECISION_EXPLANATION\", \"STATISTIC_CLARIFICATION\", \"NAME_CONFIRMATION\",\n",
    "  \"TIME_INQUIRY\", \"DURATION_QUESTION\", \"LOCATION_QUESTION\", \"REASON_INQUIRY\", \"HOW_QUESTION\",\n",
    "  \"WHY_QUESTION\", \"WHEN_QUESTION\", \"WHERE_QUESTION\", \"WHO_QUESTION\", \"WHAT_QUESTION\",\n",
    "  \"GREETING\", \"FAREWELL\", \"THANKS\", \"APOLOGY\", \"AGREEMENT\", \"DISAGREEMENT\", \"CONFIRMATION\",\n",
    "  \"NEGATION\", \"ACKNOWLEDGEMENT\", \"COMPLIMENT\", \"COMPLAINT\", \"SUGGESTION\", \"INVITATION\",\n",
    "  \"CHALLENGE\", \"DEBATE\", \"ARGUMENT\", \"DISCUSSION\",\n",
    "  \"BETTING_TIP\", \"ODDS_INQUIRY\", \"FANTASY_ADVICE\", \"LINEUP_SUGGESTION\", \"CAPTAIN_CHOICE\",\n",
    "  \"TRANSFER_RECOMMENDATION\", \"PRICE_CHANGE_ALERT\", \"POINTS_PREDICTION\", \"RISK_ASSESSMENT\",\n",
    "  \"RULE_EXPLANATION\", \"TACTIC_EXPLANATION\", \"HISTORY_LESSON\", \"PLAYER_BIOGRAPHY\",\n",
    "  \"TEAM_HISTORY\", \"COMPETITION_FORMAT\", \"OFFSIDE_EXPLANATION\", \"VAR_EXPLANATION\",\n",
    "  \"TERMINOLOGY_DEFINITION\", \"CONCEPT_CLARIFICATION\",\n",
    "  \"UNKNOWN_SOCCER_INTENT\", \"UNCLEAR_INTENT\", \"MIXED_INTENT\", \"AMBIGUOUS_INTENT\"\n",
    "]\n",
    "\n",
    "**SOCCER-SPECIFIC ANNOTATION GUIDELINES:**\n",
    "- **Player Names:** Tag complete names (e.g., \"Mario Balotelli\" = B-PLAYER_NAME I-PLAYER_NAME)\n",
    "- **Team Names:** Include full team names and nicknames (e.g., \"Manchester City\", \"Azzurri\")\n",
    "- **Match Events:** Identify goals, penalties, saves, cards, substitutions, etc.\n",
    "- **Positions:** Recognize goalkeeper, defender, midfielder, striker, captain, etc.\n",
    "- **Match Information:** Time references, scores, match phases (shootout, penalties, etc.)\n",
    "- **Venues:** Stadium names, locations\n",
    "- **Officials:** Referee names, VAR decisions\n",
    "- **Tactical Terms:** Formations, strategies, playing styles\n",
    "- **Emotional Commentary:** Celebrations, disappointments, excitement markers\n",
    "\n",
    "**Example Input String 1 (Live Penalty Commentary):**\n",
    "\"mario balotelli faces his manchester city teammate and he coolly slots it past joe hart\"\n",
    "\n",
    "**CORRECT Example Output 1:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"mario balotelli faces his manchester city teammate and he coolly slots it past joe hart\",\n",
    "    \"tokens\": [\"mario\", \"balotelli\", \"faces\", \"his\", \"manchester\", \"city\", \"teammate\", \"and\", \"he\", \"coolly\", \"slots\", \"it\", \"past\", \"joe\", \"hart\"],\n",
    "    \"tags\": [\"B-PLAYER_NAME\", \"I-PLAYER_NAME\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-TEAM_NAME\", \"I-TEAM_NAME\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-GOAL\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-PLAYER_NAME\", \"I-PLAYER_NAME\"],\n",
    "    \"intent\": \"LIVE_COMMENTARY\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Example Input String 2 (Captain Performance):**\n",
    "\"steven gerrard brilliantly done from the captain absolutely no mistake\"\n",
    "\n",
    "**CORRECT Example Output 2:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"steven gerrard brilliantly done from the captain absolutely no mistake\",\n",
    "    \"tokens\": [\"steven\", \"gerrard\", \"brilliantly\", \"done\", \"from\", \"the\", \"captain\", \"absolutely\", \"no\", \"mistake\"],\n",
    "    \"tags\": [\"B-PLAYER_NAME\", \"I-PLAYER_NAME\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-CAPTAIN\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\"],\n",
    "    \"intent\": \"LIVE_COMMENTARY\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Example Input String 3 (Match Outcome):**\n",
    "\"italy are into the semi-finals england eliminated after dominating the match\"\n",
    "\n",
    "**CORRECT Example Output 3:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"italy are into the semi-finals england eliminated after dominating the match\",\n",
    "    \"tokens\": [\"italy\", \"are\", \"into\", \"the\", \"semi-finals\", \"england\", \"eliminated\", \"after\", \"dominating\", \"the\", \"match\"],\n",
    "    \"tags\": [\"B-TEAM_NAME\", \"B-MATCH_RESULT\", \"I-MATCH_RESULT\", \"I-MATCH_RESULT\", \"B-SEMI_FINAL\", \"B-TEAM_NAME\", \"B-MATCH_RESULT\", \"B-MATCH_REVIEW\", \"I-MATCH_REVIEW\", \"I-MATCH_REVIEW\", \"I-MATCH_REVIEW\"],\n",
    "    \"intent\": \"MATCH_RESULT\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**SPECIAL CONSIDERATIONS FOR SOCCER COMMENTARY:**\n",
    "1. **Live Action:** Fast-paced commentary with emotional intensity\n",
    "2. **Technical Terms:** Soccer-specific jargon and terminology\n",
    "3. **Multiple Entities:** Player names, team names, and match events often appear together\n",
    "4. **Temporal References:** Time-sensitive information (half-time, injury time, etc.)\n",
    "5. **Emotional Language:** Excitement, disappointment, surprise in commentary\n",
    "6. **Abbreviations:** Common soccer abbreviations (VAR, PK, etc.)\n",
    "7. **Multiple Languages:** Some foreign player/team names may appear\n",
    "8. **Context Switching:** Commentary can shift between different aspects rapidly\n",
    "\n",
    "**CRITICAL REMINDERS:**\n",
    "- Always maintain BIO tagging consistency within entity phrases\n",
    "- Every token must receive exactly one tag\n",
    "- Prioritize soccer-specific entities over generic intent tags\n",
    "- Consider the commentary context when determining overall intent\n",
    "- Handle punctuation appropriately with `O` tags where no specific meaning applies\n",
    "- For compound entities (e.g., \"penalty shootout\"), tag as B-PENALTY I-PENALTY or use the most specific available entity type\n",
    "- Numbers in scores should be tagged with the score entity (e.g., \"2-1\" as B-MATCH_SCORE I-MATCH_SCORE I-MATCH_SCORE)\n",
    "\n",
    "**NOW ANNOTATE THE FOLLOWING SENTENCES:**\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Save to file\n",
    "with open(\"soccer_prompt.txt\", \"w\") as f:\n",
    "    f.write(soccer_prompt)\n",
    "\n",
    "print(\"Prompt saved to soccer_prompt.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039a8cc",
   "metadata": {},
   "source": [
    "request using directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c051359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Read the prompt from file\n",
    "with open(\"soccer_prompt.txt\") as f:\n",
    "    custom_prompt = f.read()\n",
    "\n",
    "# Build payload\n",
    "payload = {\n",
    "    \"user_id\": \"user_123\",\n",
    "    # \"gcs_path\": \"gs://stream2action-audio/youtube-videos/soccer_data/England_v_Italy_-_Watch_the_full_2012_penalty_shoot-out_16k.wav\",\n",
    "    \"gcs_path\": \"gs://stream2action-audio/youtube-videos/soccer_data\",\n",
    "    \"model_choice\": \"gemini\", #usign 2.0-flash for transcription and annotation both\n",
    "    \"output_jsonl_path\": \"/home/dchauhan/workspace/meta-asr/data_processing/hello\",\n",
    "    \"annotations\": [\"entity\", \"intent\"],\n",
    "    \"prompt\": custom_prompt\n",
    "}\n",
    "\n",
    "# Send request (adjust URL to your FastAPI server)\n",
    "url = \"http://localhost:8000/process_gcs_directory/\"\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Pretty-print the response\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeca306",
   "metadata": {},
   "source": [
    "merge whole stuff into one dataset, before pushing into hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb012a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created dataset for HIGHLIGHTS_Real_Madrid_4-0_Osasuna_LaLiga_2024_25_16k\n",
      "✅ Created dataset for England_v_Italy_-_Watch_the_full_2012_penalty_shoot-out_16k\n",
      "✅ Created combined dataset with 16 total segments\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Input: root folder containing all large audio dirs ===\n",
    "base_dir = Path(\"/home/dchauhan/workspace/meta-asr/data_processing/hello\")\n",
    "\n",
    "# === Output: where final datasets will be stored ===\n",
    "final_root = Path(\"/home/dchauhan/workspace/meta-asr/final_datasets\")\n",
    "combined_dir = final_root / \"combined_dataset\"\n",
    "combined_audio_dir = combined_dir / \"audio\"\n",
    "\n",
    "final_root.mkdir(exist_ok=True)\n",
    "combined_dir.mkdir(exist_ok=True)\n",
    "combined_audio_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Hold all combined annotations\n",
    "all_annotations = []\n",
    "grouped_annotations = defaultdict(list)\n",
    "\n",
    "# === Scan base_dir for all large audio files ===\n",
    "for large_file_dir in base_dir.iterdir():\n",
    "    if not large_file_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    segments_dir = large_file_dir / \"segments\"\n",
    "    annotations_file = large_file_dir / \"annotations.jsonl\"\n",
    "\n",
    "    # Skip if either segments or annotations are missing\n",
    "    if not segments_dir.exists() or not annotations_file.exists():\n",
    "        print(f\"⚠️ Skipping {large_file_dir.name} — missing segments or annotations.jsonl\")\n",
    "        continue\n",
    "\n",
    "    dataset_name = large_file_dir.name\n",
    "    dataset_dir = final_root / dataset_name\n",
    "    audio_out_dir = dataset_dir / \"audio\"\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "    audio_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read annotations.jsonl\n",
    "    with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        annotations = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    # Process each record in annotations\n",
    "    for record in annotations:\n",
    "        wav_file = Path(record[\"audio_filepath\"])\n",
    "\n",
    "        # Copy to individual dataset\n",
    "        shutil.copy(wav_file, audio_out_dir / wav_file.name)\n",
    "        record_individual = record.copy()\n",
    "        record_individual[\"audio_filepath\"] = f\"audio/{wav_file.name}\"\n",
    "        grouped_annotations[record[\"original_gcs_path\"]].append(record_individual)\n",
    "\n",
    "        # Copy to combined dataset\n",
    "        combined_wav_path = combined_audio_dir / wav_file.name\n",
    "        if not combined_wav_path.exists():\n",
    "            shutil.copy(wav_file, combined_wav_path)\n",
    "\n",
    "        record_combined = record.copy()\n",
    "        record_combined[\"audio_filepath\"] = f\"audio/{wav_file.name}\"\n",
    "        all_annotations.append(record_combined)\n",
    "\n",
    "    # Save per-file annotations.jsonl\n",
    "    with open(dataset_dir / \"annotations.jsonl\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in grouped_annotations[annotations[0][\"original_gcs_path\"]]:\n",
    "            f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Created dataset for {dataset_name}\")\n",
    "\n",
    "# === Save combined dataset ===\n",
    "with open(combined_dir / \"annotations.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in all_annotations:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(combined_dir / \"grouped_annotations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grouped_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Created combined dataset with {len(all_annotations)} total segments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5acb3",
   "metadata": {},
   "source": [
    "push to hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6726f3b",
   "metadata": {},
   "source": [
    "use in cli: huggingface-cli login\n",
    "with access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f24248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dchauhan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-68956972-197d43fa063ef38661897e1d;868617fa-df45-4b16-9c8a-76dfdc7ca409)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"01adv\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py:943\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ====== PUSH COMBINED DATASET ======\u001b[39;00m\n\u001b[1;32m     12\u001b[0m repo_id_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHF_USER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/audio_combined_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m upload_folder(\n\u001b[1;32m     17\u001b[0m     folder_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(combined_dir),\n\u001b[1;32m     18\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id_combined,\n\u001b[1;32m     20\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Combined dataset pushed to: https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id_combined\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3762\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3760\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RepoUrl(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3761\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError:\n\u001b[0;32m-> 3762\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m   3763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3764\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3749\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3746\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3749\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3751\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3752\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:473\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    468\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    472\u001b[0m     )\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    476\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-68956972-197d43fa063ef38661897e1d;868617fa-df45-4b16-9c8a-76dfdc7ca409)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"01adv\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "final_root = Path(\"/home/dchauhan/workspace/meta-asr/final_datasets\")\n",
    "combined_dir = final_root / \"combined_dataset\"\n",
    "\n",
    "# Your Hugging Face username or org\n",
    "HF_USER = \"01adv\"  \n",
    "\n",
    "# ====== PUSH COMBINED DATASET ======\n",
    "repo_id_combined = f\"{HF_USER}/audio_combined_dataset\"\n",
    "\n",
    "create_repo(repo_id_combined, private=True, exist_ok=True)\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=str(combined_dir),\n",
    "    path_in_repo=\"\",\n",
    "    repo_id=repo_id_combined,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Combined dataset pushed to: https://huggingface.co/datasets/{repo_id_combined}\")\n",
    "\n",
    "# ====== PUSH INDIVIDUAL DATASETS ======\n",
    "for dataset_dir in final_root.iterdir():\n",
    "    if not dataset_dir.is_dir() or dataset_dir.name == \"combined_dataset\":\n",
    "        continue\n",
    "    \n",
    "    repo_id_individual = f\"{HF_USER}/{dataset_dir.name}\"\n",
    "    create_repo(repo_id_individual, private=True, exist_ok=True)\n",
    "    \n",
    "    upload_folder(\n",
    "        folder_path=str(dataset_dir),\n",
    "        path_in_repo=\"\",\n",
    "        repo_id=repo_id_individual,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"✅ {dataset_dir.name} pushed to: https://huggingface.co/datasets/{repo_id_individual}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

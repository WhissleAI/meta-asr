{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d6bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt saved to soccer_prompt.txt\n"
     ]
    }
   ],
   "source": [
    "soccer_prompt = \"\"\"\n",
    "You are an expert linguistic annotator specializing in soccer (football) commentary and audio transcripts.\n",
    "You will receive a list of English sentences from soccer match commentary, which may include multiple sentences in a single string. Each input is a raw lowercase transcription from live match commentary, post-match analysis, or soccer-related discussions.\n",
    "\n",
    "Your task is crucial and requires precision for soccer domain understanding. For each input string, you must:\n",
    "\n",
    "1. **TOKENIZE:** Split the input into individual words and punctuation (tokens), preserving all elements including soccer-specific terminology, player names, team names, and match events.\n",
    "\n",
    "2. **ASSIGN BIO TAGS:** For each token, assign exactly one BIO tag with soccer domain expertise:\n",
    "   * **SOCCER ENTITY TAGS (Priority):** Identify soccer-specific entities using the provided `ENTITY_TYPES` list.\n",
    "     - `B-<ENTITY_TYPE>` for the *beginning* of an entity phrase (e.g., `B-PLAYER_NAME`, `B-TEAM_NAME`, `B-GOAL`).\n",
    "     - `I-<ENTITY_TYPE>` for *inside* an entity phrase (e.g., `I-PLAYER_NAME`, `I-TEAM_NAME`).\n",
    "   * **COMMENTARY INTENT TAGS (Default/Fallback):** If a token is *not* part of any specific entity, tag it to reflect the overall commentary intent.\n",
    "     - The first non-entity token of the input should be `B-<UTTERANCE_INTENT>`.\n",
    "     - Subsequent non-entity tokens should be `I-<UTTERANCE_INTENT>`.\n",
    "     - The `<UTTERANCE_INTENT>` should be from `INTENT_TYPES`.\n",
    "   * **CRITICAL:** Every token, including punctuation, must have a tag. Use `O` (Outside) if no entity or intent applies.\n",
    "\n",
    "3. **EXTRACT INTENT:** Determine and provide the single overall `intent` of the entire input string from `INTENT_TYPES`, considering the soccer context (e.g., live commentary, analysis, celebration, etc.).\n",
    "\n",
    "4. **OUTPUT FORMAT (CRITICAL):** Return a JSON array of objects. Each object must contain:\n",
    "   * `text`: The original lowercase input string (for verification).\n",
    "   * `tokens`: A JSON array of all tokenized words and punctuation.\n",
    "   * `tags`: A JSON array of BIO tags, exactly matching the `tokens` array in length.\n",
    "   * `intent`: A single string representing the overall commentary intent.\n",
    "\n",
    "**SOCCER ENTITY TYPES LIST (USE ONLY THESE FOR ENTITY TAGS):**\n",
    "[\n",
    "  \"PLAYER_NAME\", \"TEAM_NAME\", \"COACH_NAME\", \"MANAGER_NAME\", \"REFEREE_NAME\", \"ASSISTANT_REFEREE\", \"VAR_REFEREE\",\n",
    "  \"FOURTH_OFFICIAL\", \"GOALKEEPER\", \"DEFENDER\", \"MIDFIELDER\", \"FORWARD\", \"STRIKER\", \"WINGER\", \"CAPTAIN\",\n",
    "  \"SUBSTITUTE\", \"ACADEMY_PLAYER\", \"YOUTH_PLAYER\", \"VETERAN\", \"LEGEND\", \"CLUB_PRESIDENT\", \"DIRECTOR\",\n",
    "  \"GOAL\", \"ASSIST\", \"SHOT\", \"SHOT_ON_TARGET\", \"SHOT_OFF_TARGET\", \"BLOCKED_SHOT\", \"SAVE\", \"CATCH\", \"PUNCH\",\n",
    "  \"YELLOW_CARD\", \"RED_CARD\", \"SECOND_YELLOW\", \"FOUL\", \"PENALTY\", \"PENALTY_MISS\", \"PENALTY_SAVE\", \"OFFSIDE\",\n",
    "  \"SUBSTITUTION\", \"CORNER_KICK\", \"FREE_KICK\", \"DIRECT_FREE_KICK\", \"INDIRECT_FREE_KICK\", \"THROW_IN\",\n",
    "  \"KICK_OFF\", \"OWN_GOAL\", \"HEADER\", \"VOLLEY\", \"BICYCLE_KICK\", \"TACKLE\", \"INTERCEPTION\", \"CLEARANCE\",\n",
    "  \"CROSS\", \"PASS\", \"THROUGH_BALL\", \"BACK_PASS\", \"DRIBBLE\", \"NUTMEG\", \"SKILL_MOVE\", \"RUN\", \"SPRINT\",\n",
    "  \"MATCH_DATE\", \"MATCH_TIME\", \"KICK_OFF_TIME\", \"STADIUM_NAME\", \"VENUE\", \"CAPACITY\", \"ATTENDANCE\",\n",
    "  \"MATCH_SCORE\", \"FINAL_SCORE\", \"HALF_TIME_SCORE\", \"FULL_TIME\", \"HALF_TIME\", \"FIRST_HALF\", \"SECOND_HALF\",\n",
    "  \"EXTRA_TIME\", \"INJURY_TIME\", \"STOPPAGE_TIME\", \"OVERTIME\", \"ADDED_TIME\", \"MATCH_DURATION\",\n",
    "  \"LEAGUE_NAME\", \"TOURNAMENT_NAME\", \"COMPETITION\", \"CHAMPIONSHIP\", \"CUP\", \"FRIENDLY\", \"INTERNATIONAL\",\n",
    "  \"DOMESTIC\", \"CONTINENTAL\", \"WORLD_CUP\", \"EUROS\", \"CHAMPIONS_LEAGUE\", \"EUROPA_LEAGUE\", \"PREMIER_LEAGUE\",\n",
    "  \"LA_LIGA\", \"SERIE_A\", \"BUNDESLIGA\", \"LIGUE_1\", \"MLS\", \"COPA_AMERICA\", \"AFCON\",\n",
    "  \"FORMATION\", \"LINEUP\", \"STARTING_XI\", \"BENCH\", \"SQUAD\", \"TACTIC\", \"STRATEGY\", \"GAME_PLAN\",\n",
    "  \"PRESSING\", \"COUNTER_ATTACK\", \"POSSESSION\", \"PARKING_THE_BUS\", \"HIGH_LINE\", \"LOW_BLOCK\",\n",
    "  \"MATCH_RESULT\", \"WIN\", \"LOSS\", \"DRAW\", \"VICTORY\", \"DEFEAT\", \"TIE\", \"POINTS\", \"RANKING\", \"TABLE_POSITION\",\n",
    "  \"LEAGUE_POSITION\", \"GOAL_DIFFERENCE\", \"GOALS_FOR\", \"GOALS_AGAINST\", \"CLEAN_SHEET\", \"HAT_TRICK\",\n",
    "  \"BRACE\", \"POSSESSION_PERCENTAGE\", \"PASS_ACCURACY\", \"SHOTS_ON_TARGET\", \"CORNERS\", \"FOULS_COMMITTED\",\n",
    "  \"SEASON\", \"FIXTURE\", \"MATCH_DAY\", \"GAME_WEEK\", \"ROUND\", \"GROUP\", \"GROUP_STAGE\", \"KNOCKOUT_STAGE\",\n",
    "  \"QUARTER_FINAL\", \"SEMI_FINAL\", \"FINAL\", \"PLAYOFF\", \"RELEGATION\", \"PROMOTION\", \"TRANSFER_WINDOW\",\n",
    "  \"HOME_TEAM\", \"AWAY_TEAM\", \"HOME_GROUND\", \"AWAY_GROUND\", \"NEUTRAL_VENUE\", \"TRAINING_GROUND\",\n",
    "  \"ACADEMY\", \"CLUB_FACILITY\", \"DRESSING_ROOM\", \"TUNNEL\", \"PITCH\", \"GRASS\", \"ARTIFICIAL_TURF\",\n",
    "  \"BALL\", \"GOAL_POST\", \"CROSSBAR\", \"NET\", \"JERSEY\", \"BOOTS\", \"SHIN_GUARDS\", \"GLOVES\",\n",
    "  \"VAR\", \"GOAL_LINE_TECHNOLOGY\", \"HAWK_EYE\", \"OFFSIDE_LINE\", \"PENALTY_AREA\", \"SIX_YARD_BOX\",\n",
    "  \"CENTER_CIRCLE\", \"CORNER_ARC\", \"TOUCHLINE\", \"GOAL_LINE\",\n",
    "  \"INJURY\", \"INJURY_TIME_OUT\", \"MEDICAL_TIMEOUT\", \"STRETCHER\", \"CONCUSSION\", \"HAMSTRING\",\n",
    "  \"ANKLE\", \"KNEE\", \"HEAD_INJURY\", \"FITNESS\", \"STAMINA\", \"PACE\", \"STRENGTH\", \"AGILITY\",\n",
    "  \"TRANSFER\", \"LOAN\", \"CONTRACT\", \"SIGNING\", \"RELEASE_CLAUSE\", \"TRANSFER_FEE\", \"WAGE\",\n",
    "  \"AGENT\", \"NEGOTIATION\", \"MEDICAL_EXAMINATION\", \"ANNOUNCEMENT\",\n",
    "  \"GOLDEN_BOOT\", \"GOLDEN_BALL\", \"PLAYER_OF_THE_MATCH\", \"PLAYER_OF_THE_SEASON\", \"BALLON_DOR\",\n",
    "  \"ROOKIE_OF_THE_YEAR\", \"COACH_OF_THE_YEAR\", \"FAIR_PLAY_AWARD\", \"TOP_SCORER\", \"MOST_ASSISTS\",\n",
    "  \"COMMENTATOR\", \"ANALYST\", \"PUNDIT\", \"BROADCAST\", \"LIVE_STREAM\", \"HIGHLIGHTS\", \"REPLAY\",\n",
    "  \"SLOW_MOTION\", \"CAMERA_ANGLE\", \"MICROPHONE\", \"INTERVIEW\", \"POST_MATCH\", \"PRE_MATCH\",\n",
    "  \"FAN\", \"SUPPORTER\", \"ULTRAS\", \"CHANT\", \"SONG\", \"SCARF\", \"FLAG\", \"BANNER\", \"TIFO\",\n",
    "  \"AWAY_FANS\", \"HOME_FANS\", \"ATMOSPHERE\", \"STADIUM_ATMOSPHERE\", \"CROWD\", \"NOISE\"\n",
    "]\n",
    "\n",
    "**SOCCER INTENT TYPES LIST (USE ONE FOR UTTERANCE INTENT AND FOR DEFAULT TAGS):**\n",
    "[\n",
    "  \"MATCH_INQUIRY\", \"SCORE_REQUEST\", \"PLAYER_STATS_REQUEST\", \"TEAM_INFO_REQUEST\", \"FIXTURE_INQUIRY\",\n",
    "  \"TABLE_POSITION_REQUEST\", \"LEAGUE_STANDINGS_REQUEST\", \"TRANSFER_NEWS_REQUEST\", \"INJURY_UPDATE_REQUEST\",\n",
    "  \"HISTORICAL_DATA_REQUEST\", \"RECORD_INQUIRY\", \"COMPARISON_REQUEST\", \"PREDICTION_REQUEST\",\n",
    "  \"LIVE_COMMENTARY\", \"GOAL_ANNOUNCEMENT\", \"CARD_ANNOUNCEMENT\", \"SUBSTITUTION_ANNOUNCEMENT\",\n",
    "  \"INJURY_UPDATE\", \"SCORE_UPDATE\", \"HALF_TIME_UPDATE\", \"FULL_TIME_UPDATE\", \"MATCH_EVENT_UPDATE\",\n",
    "  \"VAR_DECISION\", \"REFEREE_DECISION\", \"WEATHER_UPDATE\", \"ATTENDANCE_UPDATE\",\n",
    "  \"TACTICAL_ANALYSIS\", \"PLAYER_PERFORMANCE_ANALYSIS\", \"TEAM_PERFORMANCE_ANALYSIS\", \"MATCH_REVIEW\",\n",
    "  \"SEASON_REVIEW\", \"PREDICTION\", \"OPINION\", \"CRITICISM\", \"PRAISE\", \"EVALUATION\", \"ASSESSMENT\",\n",
    "  \"COMPARISON\", \"RANKING\", \"RATING\", \"RECOMMENDATION\",\n",
    "  \"TRANSFER_NEWS\", \"INJURY_NEWS\", \"CONTRACT_NEWS\", \"COACHING_CHANGE\", \"TEAM_NEWS\",\n",
    "  \"LEAGUE_UPDATE\", \"RULE_CHANGE\", \"DISCIPLINARY_ACTION\", \"FINE_ANNOUNCEMENT\", \"SUSPENSION_NEWS\",\n",
    "  \"AWARD_ANNOUNCEMENT\", \"MILESTONE_ANNOUNCEMENT\", \"RETIREMENT_NEWS\", \"DEBUT_ANNOUNCEMENT\",\n",
    "  \"CELEBRATION\", \"EXCITEMENT\", \"DISAPPOINTMENT\", \"FRUSTRATION\", \"ENCOURAGEMENT\", \"MOTIVATION\",\n",
    "  \"CHANT\", \"SING_ALONG\", \"CROWD_PARTICIPATION\", \"FAN_REACTION\", \"EMOTIONAL_EXPRESSION\",\n",
    "  \"SURPRISE\", \"SHOCK\", \"AMAZEMENT\", \"DISBELIEF\",\n",
    "  \"TACTICAL_INSTRUCTION\", \"COACHING_COMMAND\", \"REFEREE_INSTRUCTION\", \"CROWD_DIRECTION\",\n",
    "  \"BROADCAST_INSTRUCTION\", \"CAMERA_DIRECTION\", \"REPLAY_REQUEST\", \"HIGHLIGHT_REQUEST\",\n",
    "  \"VOLUME_CONTROL\", \"CHANNEL_CHANGE\", \"MUTE_REQUEST\",\n",
    "  \"RULE_CLARIFICATION\", \"DECISION_EXPLANATION\", \"STATISTIC_CLARIFICATION\", \"NAME_CONFIRMATION\",\n",
    "  \"TIME_INQUIRY\", \"DURATION_QUESTION\", \"LOCATION_QUESTION\", \"REASON_INQUIRY\", \"HOW_QUESTION\",\n",
    "  \"WHY_QUESTION\", \"WHEN_QUESTION\", \"WHERE_QUESTION\", \"WHO_QUESTION\", \"WHAT_QUESTION\",\n",
    "  \"GREETING\", \"FAREWELL\", \"THANKS\", \"APOLOGY\", \"AGREEMENT\", \"DISAGREEMENT\", \"CONFIRMATION\",\n",
    "  \"NEGATION\", \"ACKNOWLEDGEMENT\", \"COMPLIMENT\", \"COMPLAINT\", \"SUGGESTION\", \"INVITATION\",\n",
    "  \"CHALLENGE\", \"DEBATE\", \"ARGUMENT\", \"DISCUSSION\",\n",
    "  \"BETTING_TIP\", \"ODDS_INQUIRY\", \"FANTASY_ADVICE\", \"LINEUP_SUGGESTION\", \"CAPTAIN_CHOICE\",\n",
    "  \"TRANSFER_RECOMMENDATION\", \"PRICE_CHANGE_ALERT\", \"POINTS_PREDICTION\", \"RISK_ASSESSMENT\",\n",
    "  \"RULE_EXPLANATION\", \"TACTIC_EXPLANATION\", \"HISTORY_LESSON\", \"PLAYER_BIOGRAPHY\",\n",
    "  \"TEAM_HISTORY\", \"COMPETITION_FORMAT\", \"OFFSIDE_EXPLANATION\", \"VAR_EXPLANATION\",\n",
    "  \"TERMINOLOGY_DEFINITION\", \"CONCEPT_CLARIFICATION\",\n",
    "  \"UNKNOWN_SOCCER_INTENT\", \"UNCLEAR_INTENT\", \"MIXED_INTENT\", \"AMBIGUOUS_INTENT\"\n",
    "]\n",
    "\n",
    "**SOCCER-SPECIFIC ANNOTATION GUIDELINES:**\n",
    "- **Player Names:** Tag complete names (e.g., \"Mario Balotelli\" = B-PLAYER_NAME I-PLAYER_NAME)\n",
    "- **Team Names:** Include full team names and nicknames (e.g., \"Manchester City\", \"Azzurri\")\n",
    "- **Match Events:** Identify goals, penalties, saves, cards, substitutions, etc.\n",
    "- **Positions:** Recognize goalkeeper, defender, midfielder, striker, captain, etc.\n",
    "- **Match Information:** Time references, scores, match phases (shootout, penalties, etc.)\n",
    "- **Venues:** Stadium names, locations\n",
    "- **Officials:** Referee names, VAR decisions\n",
    "- **Tactical Terms:** Formations, strategies, playing styles\n",
    "- **Emotional Commentary:** Celebrations, disappointments, excitement markers\n",
    "\n",
    "**Example Input String 1 (Live Penalty Commentary):**\n",
    "\"mario balotelli faces his manchester city teammate and he coolly slots it past joe hart\"\n",
    "\n",
    "**CORRECT Example Output 1:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"mario balotelli faces his manchester city teammate and he coolly slots it past joe hart\",\n",
    "    \"tokens\": [\"mario\", \"balotelli\", \"faces\", \"his\", \"manchester\", \"city\", \"teammate\", \"and\", \"he\", \"coolly\", \"slots\", \"it\", \"past\", \"joe\", \"hart\"],\n",
    "    \"tags\": [\"B-PLAYER_NAME\", \"I-PLAYER_NAME\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-TEAM_NAME\", \"I-TEAM_NAME\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-GOAL\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-PLAYER_NAME\", \"I-PLAYER_NAME\"],\n",
    "    \"intent\": \"LIVE_COMMENTARY\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Example Input String 2 (Captain Performance):**\n",
    "\"steven gerrard brilliantly done from the captain absolutely no mistake\"\n",
    "\n",
    "**CORRECT Example Output 2:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"steven gerrard brilliantly done from the captain absolutely no mistake\",\n",
    "    \"tokens\": [\"steven\", \"gerrard\", \"brilliantly\", \"done\", \"from\", \"the\", \"captain\", \"absolutely\", \"no\", \"mistake\"],\n",
    "    \"tags\": [\"B-PLAYER_NAME\", \"I-PLAYER_NAME\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"B-CAPTAIN\", \"B-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\", \"I-LIVE_COMMENTARY\"],\n",
    "    \"intent\": \"LIVE_COMMENTARY\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Example Input String 3 (Match Outcome):**\n",
    "\"italy are into the semi-finals england eliminated after dominating the match\"\n",
    "\n",
    "**CORRECT Example Output 3:**\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"text\": \"italy are into the semi-finals england eliminated after dominating the match\",\n",
    "    \"tokens\": [\"italy\", \"are\", \"into\", \"the\", \"semi-finals\", \"england\", \"eliminated\", \"after\", \"dominating\", \"the\", \"match\"],\n",
    "    \"tags\": [\"B-TEAM_NAME\", \"B-MATCH_RESULT\", \"I-MATCH_RESULT\", \"I-MATCH_RESULT\", \"B-SEMI_FINAL\", \"B-TEAM_NAME\", \"B-MATCH_RESULT\", \"B-MATCH_REVIEW\", \"I-MATCH_REVIEW\", \"I-MATCH_REVIEW\", \"I-MATCH_REVIEW\"],\n",
    "    \"intent\": \"MATCH_RESULT\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**SPECIAL CONSIDERATIONS FOR SOCCER COMMENTARY:**\n",
    "1. **Live Action:** Fast-paced commentary with emotional intensity\n",
    "2. **Technical Terms:** Soccer-specific jargon and terminology\n",
    "3. **Multiple Entities:** Player names, team names, and match events often appear together\n",
    "4. **Temporal References:** Time-sensitive information (half-time, injury time, etc.)\n",
    "5. **Emotional Language:** Excitement, disappointment, surprise in commentary\n",
    "6. **Abbreviations:** Common soccer abbreviations (VAR, PK, etc.)\n",
    "7. **Multiple Languages:** Some foreign player/team names may appear\n",
    "8. **Context Switching:** Commentary can shift between different aspects rapidly\n",
    "\n",
    "**CRITICAL REMINDERS:**\n",
    "- Always maintain BIO tagging consistency within entity phrases\n",
    "- Every token must receive exactly one tag\n",
    "- Prioritize soccer-specific entities over generic intent tags\n",
    "- Consider the commentary context when determining overall intent\n",
    "- Handle punctuation appropriately with `O` tags where no specific meaning applies\n",
    "- For compound entities (e.g., \"penalty shootout\"), tag as B-PENALTY I-PENALTY or use the most specific available entity type\n",
    "- Numbers in scores should be tagged with the score entity (e.g., \"2-1\" as B-MATCH_SCORE I-MATCH_SCORE I-MATCH_SCORE)\n",
    "\n",
    "**NOW ANNOTATE THE FOLLOWING SENTENCES:**\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Save to file\n",
    "with open(\"soccer_prompt.txt\", \"w\") as f:\n",
    "    f.write(soccer_prompt)\n",
    "\n",
    "print(\"Prompt saved to soccer_prompt.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039a8cc",
   "metadata": {},
   "source": [
    "request using directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c051359",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Pretty-print the response\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py:900\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[1;32m    896\u001b[0m             \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[1;32m    897\u001b[0m             \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[1;32m    898\u001b[0m             \u001b[38;5;66;03m# used.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Read the prompt from file\n",
    "with open(\"soccer_prompt.txt\") as f:\n",
    "    custom_prompt = f.read()\n",
    "\n",
    "# Build payload\n",
    "payload = {\n",
    "    \"user_id\": \"user_123\",\n",
    "    \"gcs_path\": \"gs://stream2action-audio/youtube-videos/soccer_data\",\n",
    "    \"model_choice\": \"gemini\", #usign 2.0-flash for transcription and annotation both\n",
    "    \"output_jsonl_path\": \"/home/dchauhan/workspace/meta-asr/data_processing/hello\",\n",
    "    \"annotations\": [\"age\", \"gender\", \"emotion\", \"entity\", \"intent\"],\n",
    "    \"prompt\": custom_prompt\n",
    "}\n",
    "\n",
    "# Send request (adjust URL to your FastAPI server)\n",
    "url = \"http://localhost:8000/process_gcs_directory/\"\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Pretty-print the response\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeca306",
   "metadata": {},
   "source": [
    "merge whole stuff into one dataset, before pushing into hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb012a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created dataset for Argentina_vs_16k\n",
      "✅ Created dataset for BULLS_at_HAWKS_FULL_GAME_HIGHLIGHTS_November_9_2024_16k\n",
      "✅ Created dataset for 76ERS_at_LAKERS_FULL_GAME_HIGHLIGHTS_November_8_2024_16k\n",
      "✅ Created combined dataset with 74 total segments\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Input: root folder containing all large audio dirs ===\n",
    "base_dir = Path(\"/home/dchauhan/workspace/meta-asr/data_processing/hello\")\n",
    "\n",
    "# === Output: where final datasets will be stored ===\n",
    "final_root = Path(\"/home/dchauhan/workspace/meta-asr/final_datasets\")\n",
    "combined_dir = final_root / \"combined_dataset\"\n",
    "combined_audio_dir = combined_dir / \"audio\"\n",
    "\n",
    "final_root.mkdir(exist_ok=True)\n",
    "combined_dir.mkdir(exist_ok=True)\n",
    "combined_audio_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Hold all combined annotations\n",
    "all_annotations = []\n",
    "grouped_annotations = defaultdict(list)\n",
    "\n",
    "# === Scan base_dir for all large audio files ===\n",
    "for large_file_dir in base_dir.iterdir():\n",
    "    if not large_file_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    segments_dir = large_file_dir / \"segments\"\n",
    "    annotations_file = large_file_dir / \"annotations.jsonl\"\n",
    "\n",
    "    # Skip if either segments or annotations are missing\n",
    "    if not segments_dir.exists() or not annotations_file.exists():\n",
    "        print(f\"⚠️ Skipping {large_file_dir.name} — missing segments or annotations.jsonl\")\n",
    "        continue\n",
    "\n",
    "    dataset_name = large_file_dir.name\n",
    "    dataset_dir = final_root / dataset_name\n",
    "    audio_out_dir = dataset_dir / \"audio\"\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "    audio_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read annotations.jsonl\n",
    "    with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        annotations = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    # Process each record in annotations\n",
    "    for record in annotations:\n",
    "        wav_file = Path(record[\"audio_filepath\"])\n",
    "\n",
    "        # Copy to individual dataset\n",
    "        shutil.copy(wav_file, audio_out_dir / wav_file.name)\n",
    "        record_individual = record.copy()\n",
    "        record_individual[\"audio_filepath\"] = f\"audio/{wav_file.name}\"\n",
    "        grouped_annotations[record[\"original_gcs_path\"]].append(record_individual)\n",
    "\n",
    "        # Copy to combined dataset\n",
    "        combined_wav_path = combined_audio_dir / wav_file.name\n",
    "        if not combined_wav_path.exists():\n",
    "            shutil.copy(wav_file, combined_wav_path)\n",
    "\n",
    "        record_combined = record.copy()\n",
    "        record_combined[\"audio_filepath\"] = f\"audio/{wav_file.name}\"\n",
    "        all_annotations.append(record_combined)\n",
    "\n",
    "    # Save per-file annotations.jsonl\n",
    "    with open(dataset_dir / \"annotations.jsonl\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in grouped_annotations[annotations[0][\"original_gcs_path\"]]:\n",
    "            f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Created dataset for {dataset_name}\")\n",
    "\n",
    "# === Save combined dataset ===\n",
    "with open(combined_dir / \"annotations.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in all_annotations:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(combined_dir / \"grouped_annotations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grouped_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Created combined dataset with {len(all_annotations)} total segments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5acb3",
   "metadata": {},
   "source": [
    "push to hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6726f3b",
   "metadata": {},
   "source": [
    "use hugging face\n",
    "with access token in env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f24248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Authenticated as: 01adv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (74 / 74)              : 100%|██████████| 71.0MB / 71.0MB,   ???B/s  \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (74 / 74)              : 100%|██████████| 71.0MB / 71.0MB,  0.00B/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "  ..._November_9_2024_16k_segment_26.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ..._November_9_2024_16k_segment_27.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ..._November_9_2024_16k_segment_28.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ..._November_9_2024_16k_segment_29.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_3.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_4.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_5.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_7.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_6.wav: 100%|██████████|  960kB /  960kB            \n",
      "  ...S_November_9_2024_16k_segment_8.wav: 100%|██████████|  960kB /  960kB            \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset pushed to: https://huggingface.co/datasets/01adv/audio_combined_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "# --- Load token from environment ---\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "HF_TOKEN = \"user your own\"\n",
    "if HF_TOKEN is None:\n",
    "    raise ValueError(\"HF_TOKEN environment variable not found!\")\n",
    "\n",
    "# --- Initialize API ---\n",
    "api = HfApi()\n",
    "\n",
    "# --- Auto-discover your namespace ---\n",
    "user_info = api.whoami(token=HF_TOKEN)\n",
    "HF_USER = user_info[\"name\"]  # this is your exact username/org\n",
    "print(f\"✅ Authenticated as: {HF_USER}\")\n",
    "\n",
    "# --- Paths ---\n",
    "final_root = Path(\"/home/dchauhan/workspace/meta-asr/final_datasets\")\n",
    "combined_dir = final_root / \"combined_dataset\"\n",
    "\n",
    "# ====== PUSH COMBINED DATASET ======\n",
    "repo_id_combined = f\"{HF_USER}/audio_combined_dataset\"\n",
    "\n",
    "api.create_repo(\n",
    "    repo_id=repo_id_combined,\n",
    "    repo_type=\"dataset\",\n",
    "    exist_ok=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=str(combined_dir),\n",
    "    path_in_repo=\"\",\n",
    "    repo_id=repo_id_combined,\n",
    "    repo_type=\"dataset\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"✅ Combined dataset pushed to: https://huggingface.co/datasets/{repo_id_combined}\")\n",
    "\n",
    "# ====== PUSH INDIVIDUAL DATASETS ======\n",
    "# only use when individual files upload is required\n",
    "\n",
    "# for dataset_dir in final_root.iterdir():\n",
    "#     if not dataset_dir.is_dir() or dataset_dir.name == \"combined_dataset\":\n",
    "#         continue\n",
    "    \n",
    "#     repo_id_individual = f\"{HF_USER}/{dataset_dir.name}\"\n",
    "#     api.create_repo(\n",
    "#         repo_id=repo_id_individual,\n",
    "#         repo_type=\"dataset\",\n",
    "#         private=True,\n",
    "#         exist_ok=True,\n",
    "#         token=HF_TOKEN\n",
    "#     )\n",
    "    \n",
    "#     upload_folder(\n",
    "#         folder_path=str(dataset_dir),\n",
    "#         path_in_repo=\"\",\n",
    "#         repo_id=repo_id_individual,\n",
    "#         repo_type=\"dataset\",\n",
    "#         token=HF_TOKEN\n",
    "#     )\n",
    "#     print(f\"✅ {dataset_dir.name} pushed to: https://huggingface.co/datasets/{repo_id_individual}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
